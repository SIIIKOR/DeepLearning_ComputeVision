{"cells":[{"cell_type":"markdown","metadata":{"id":"VoFDJCoX-Wdp"},"source":["## Normalizing Flows\n","\n","Your task is to implement a version of Normalizing Flow for image generation. Our implementation will be based on RealNVP (https://arxiv.org/pdf/1605.08803.pdf) and we will be training on one class from MNIST. Your task is to read the paper in details and implement simple version of the algorithm from the paper:\n","\n","\n","1. Implement simple CouplingLayers (see RealNVP paper) with neural networks using a few fully connected layers with hidden activations of your choice. More on the CouplingLayers can be also found in https://arxiv.org/pdf/1410.8516.pdf. Remember to implement properly logarithm of a Jacobian determinant calculation. Implement only single scale architecture, ignore multiscale architecture with masked convolution and batch normalization. (2 points)\n","2. Implement RealNVP class combining many CouplingLayers with proper masking pattern (rememeber to alternate between unmodified pixels) with forward and inverse flows. (1 points)\n","3. Implement a loss function `nf_loss` (data log-likelihood) for the model. Hint: check `torch.distributions` (1 point)\n","4. Train your model to achieve good looking samples (similar to training set - similar to that appended to assignmenmt on moodle). The training process should take between 5-10 minutes. (2 points)\n","5. Sample from your model and pick 2 images (as visually different as possible) from your samples and plot 10 images that correspond to equally spaced linear interpolations in latent space between those images you picked. (1 point)\n","6. Use method from section 5.2 from https://arxiv.org/pdf/1410.8516.pdf with trained model and inpaint 5 sampled images with different random parts of your image occluded (50% of the image must be occluded). (2 point)\n","7. Write a report describing your solution, add loss plots and samples from the model. Write which hyperparameter sets worked for you and which did not. (1 point)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"gdaZKZbAOz5K","executionInfo":{"status":"ok","timestamp":1655323427660,"user_tz":-120,"elapsed":816,"user":{"displayName":"Mateusz Sikorski","userId":"06143817599360793457"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","import numpy as np"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"rLjxcE3rCbeC","executionInfo":{"status":"ok","timestamp":1655323428401,"user_tz":-120,"elapsed":3,"user":{"displayName":"Mateusz Sikorski","userId":"06143817599360793457"}}},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"OgGe_mkhOz5R","executionInfo":{"status":"ok","timestamp":1655323428402,"user_tz":-120,"elapsed":4,"user":{"displayName":"Mateusz Sikorski","userId":"06143817599360793457"}}},"outputs":[],"source":["def plot(samples):\n","    length = len(samples)\n","    fig, ax = plt.subplots(1, length, figsize=(2*length, 2))\n","    fig.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n","    for j in range(length):\n","        ax[j].imshow(samples[j].cpu().numpy())\n","        ax[j].axis('off')\n","    plt.show()\n","\n","def get_mask1d(length=784, odd=True):\n","    mask = torch.zeros(length).to(device)\n","    for i in range(length):\n","        if i % 2 == int(odd):\n","            mask[i] = 1\n","    return mask\n","\n","\n","class CombScaleTranslationModule(nn.Module):\n","    def __init__(self, in_features, depth=2):\n","        super().__init__()\n","\n","        self.depth = depth\n","\n","        self.layers = nn.ModuleList(\n","            [\n","             nn.Linear(in_features, 2*in_features) if i == 0 else nn.Linear(2*in_features, 2*in_features)\n","             for i in range(depth)\n","            ]\n","        )\n","        \n","        self.relu = nn.ReLU()\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, x):\n","        for i, layer in enumerate(self.layers):\n","            x = layer(x)\n","            if i != self.depth-1:\n","                x = self.relu(x)\n","\n","        scale, translation = x.chunk(2, 1)\n","        return self.tanh(scale), translation\n","\n","\n","class Coupling_layer(nn.Module):\n","    odd_mask = get_mask1d()\n","\n","    def __init__(self, *args, **kwargs):\n","        super().__init__()\n","\n","        # based on layer_id mask odd and even values will oscillate\n","        self.layer_id = args[0]\n","        self.depth = 2\n","        if len(args) > 1:\n","            self.depth = args[1]\n","\n","        self.mask = self.odd_mask if self.layer_id % 2 == 1 else 1 - self.odd_mask\n","\n","        # neural nets don't change dimentions\n","        self.nn_comb_scale_translation = CombScaleTranslationModule(784, self.depth)\n","\n","        # learnable parameters for scale to reduce NaNs\n","        # self.scale_par = nn.Parameter(torch.ones(1), requires_grad=True)\n"," \n","    def forward_flow(self, x):\n","        untouched_x = self.mask * x # part of the vector that will be passed on unmodified\n","        \n","        scale, translation = self.nn_comb_scale_translation(untouched_x)\n","\n","        # scale = self.scale_par * scale\n","        # affine transformation\n","        y = (1 - self.mask) * (x * torch.exp(scale) + translation)\n","        return (\n","            untouched_x + y, # we merge vectors that were split using masks\n","            ((1 - self.mask) * scale).sum(-1) # logarithm of jacobian determinant\n","            )\n","\n","    def inverse_flow(self, z):\n","        untouched_x = self.mask * z # part of the vector that was passed on umodified\n","\n","        scale, translation = self.nn_comb_scale_translation(untouched_x)\n","\n","        # scale = self.scale_par * scale\n","        # inverse affine transformation\n","        touched_x = (1 - self.mask) * (z - translation) * torch.exp(-scale)\n","        return untouched_x + touched_x # we merge vectors that were split using masks\n","    \n","    \n","class RealNVP(nn.Module):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__()\n","\n","        self.layer_amount = args[0]\n","        self.coupling_layer_depth = 2\n","        if len(args) > 1:\n","            self.coupling_layer_depth = args[1]\n","        self.coupling_layers = nn.ModuleList(\n","            [\n","             Coupling_layer(idx, self.coupling_layer_depth)\n","             for idx in range(self.layer_amount)\n","            ]\n","        )\n","    \n","    def forward_flow(self, x):\n","        log_det_j = torch.zeros(x.shape[0])\n","\n","        for coupling_layer in self.coupling_layers:\n","            x, log_det_j = coupling_layer.forward_flow(x)\n","            log_det_j += log_det_j\n","\n","        return x, log_det_j\n","    \n","    def inverse_flow(self, z):\n","        for coupling_layer in self.coupling_layers:\n","            z = coupling_layer.inverse_flow(z)\n","\n","        return z\n","        \n","        \n","def nf_loss(z, logdetJ):\n","    pi = torch.tensor(np.pi).to(device)\n","    # minimalizuje wyrażenie z minusem czyli maksymalizuje log likelihood\n","\n","    # tutaj to poprawić z tego rozkładu z pytorch\n","    m = torch.distributions.normal.Normal(torch.tensor([0.0]).to(device), torch.tensor([1.0]).to(device))\n","\n","    # return -(torch.log(0.5 * torch.log(2 * pi) + 0.5 * z**2).sum(1) + logdetJ).mean()\n","    return -(m.log_prob(z).sum(1) + logdetJ).mean()"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"ZTx1iSUlOz5X","executionInfo":{"status":"ok","timestamp":1655323428402,"user_tz":-120,"elapsed":4,"user":{"displayName":"Mateusz Sikorski","userId":"06143817599360793457"}}},"outputs":[],"source":["dataset = torchvision.datasets.MNIST(root=r'./mnist/', \n","                                     train=True,\n","                                     transform=transforms.ToTensor(),\n","                                     download=True)\n","x = (dataset.data.float() / 255. - 0.5)\n","y = dataset.targets\n","x = x[y == 5]\n","\n","dataloader = DataLoader(x, batch_size=128, shuffle=True)"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"3qNzQZqHOz5Z","scrolled":true,"executionInfo":{"status":"ok","timestamp":1655323525343,"user_tz":-120,"elapsed":1465,"user":{"displayName":"Mateusz Sikorski","userId":"06143817599360793457"}}},"outputs":[],"source":["n_epochs = 3000\n","lr = 5e-4\n","eps = 1e-3\n","\n","model = RealNVP(10, 3).to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr, eps=eps)"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1GFhYx3mQVnZRmg2_edxDTPKy1f941tb4"},"id":"h0Kn26CoOz5b","scrolled":false,"executionInfo":{"status":"ok","timestamp":1655328677552,"user_tz":-120,"elapsed":5151837,"user":{"displayName":"Mateusz Sikorski","userId":"06143817599360793457"}},"outputId":"f533858b-ccf2-46d3-d2bd-a74dc4946c3e"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["sample_noise = torch.randn(5,28,28).to(device)\n","\n","for i in range(n_epochs):\n","    model.train()\n","    loss_acc = 0\n","    for j, x in enumerate(dataloader):\n","        optimizer.zero_grad()\n","        x = (x.float() + torch.randn(x.shape) / 64.).to(device)\n","        x = x.view(-1, 28**2)\n","        z, logdetJ = model.forward_flow(x)\n","        loss = nf_loss(z, logdetJ)\n","        loss_acc += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","    \n","    if i%10 == 0:\n","        print(f'Epoch: {i + 1}/{n_epochs} Loss: {(loss_acc / (j+1)):.4f}')\n","        with torch.no_grad():\n","            model.eval()\n","            sample_noise = sample_noise.view(-1, 28**2)\n","            samples = model.inverse_flow(sample_noise)\n","            plot(samples.view(-1, 28, 28))"]},{"cell_type":"code","source":["test_img = z.view(-1, 28, 28)\n","plot(test_img.detach())"],"metadata":{"id":"GCk9zKZ24AAF","executionInfo":{"status":"aborted","timestamp":1655323516547,"user_tz":-120,"elapsed":4,"user":{"displayName":"Mateusz Sikorski","userId":"06143817599360793457"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Normalizing Flows.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":0}